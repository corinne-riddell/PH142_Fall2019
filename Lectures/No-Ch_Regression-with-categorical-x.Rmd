---
title: "Bonus lecture: Regression modelling with a categorical exposure"
author: "Corinne Riddell"
date: "November 25, 2019"
output: pdf_document
---

### Final exam information

1. You can bring one single-sided cheat sheet to the final exam. It has the same 
constaints as outlined previously.
2. Content. Heavily focused on Part III of the course (Chapter 17-24 and the 
bootstrap, permutation and other non-parametric tests, covered after Thanksgiving). 
We will hold an inclass review session (Jeopardy) on December 6th and the GSIs will
also review content during labs on the week of Dec 2 (mistake: used to say Dec 6, but should be Dec 2).
3. Code that is fair game for writing/interpretation of the code or resulting
output: `qt()`, `pt()`, `qnorm()`, `pnorm()`, `pchisq()`, testing functions(`t.test()`, 
`binom.test()`, `prop.test()`, `chisq.test()`), 
`broom` functions (i.e., `tidy()`, `glance()`, and `augment()`), `lm()`, 
`predict()`, `confint`, `aov()`, `TukeysHSD()`
4. Code that is fair game for interpretation: `ggplot2`, `dplyr`, and may have a 
few minor points for general R intuition (e.g. what does `<-` do?)
5. We will supply the formula sheet called "Inference-formulas.pdf" which is under
Files on bCourses. You need to know when to use each of the formulas, so you might
also arrange this information on your cheatsheet in a useful way!

### Learning objectives for today

* Learn how to generalize the simple regression model to the case when the 
$x$ variable is categorical (and $y$ is still continuous)
* Learn how to run these models in R

### Example

Calcium is an essential mineral that regulates the heart, is important for blood clotting and for building healthy bones. The National Osteoporosis Foundation recommends a daily calcium intake of 1000-1200 mg/day for adult men and women. While calcium is contained in some foods, most adults do not get enough calcium in their diets and take supplements. Unfortunately some of the supplements have side effects such as gastric distress, making them difficult for some patients to take on a regular basis.  

A study is designed to test whether there is a difference in mean daily calcium intake in adults with normal bone density, adults with osteopenia (a low bone density which may lead to osteoporosis) and adults with osteoporosis. Adults 60 years of age with normal bone density, osteopenia and osteoporosis are selected at random from hospital records and invited to participate in the study. Each participant's daily calcium intake is measured based on reported food intake and supplements. The data are shown below.  

### The data

```{r, echo=F, message=F, warning=FALSE}
library(tidyverse)
```


```{r}
#students, don't worry knowing the next lines of code. Just have a look at 
#the dataset so you understand what variables it contains
normal <- c(1200, 1000, 980, 900, 750, 800)
osteopenia <- c(1000, 1100, 700, 800, 500, 700)
osteoporosis <- c(890, 650, 1100, 900, 400, 350)

calcium_data <- data.frame(calcium_intake = c(normal, osteopenia, osteoporosis), 
           type = c(rep("normal", 6), rep("osteopenia", 6), 
                    rep("osteoporosis", 6)),
           type_num = c(rep(1, 6), rep(2, 6), rep(3, 6)))

calcium_data

```

### Refresher on `lm()`

* So far, we've run linear regression when both the outcome and explanatory 
variables are continuous
* We can also use linear regression when the outcome is continuous, but the 
explanatory variable is categorical
* In today's lecture we learn how to run and interpret these sorts of models
and see how the results compare to the output from an ANOVA model.

### Running linear regression using a categorical explanatory variable

The first thing you want to do is check how the categorical variable is coded
and change the order of the categorical (factor variable) if you need to.

```{r}
str(calcium_data)
```

This shows us that `type` is coded as a factor (R's way of storing categorical data)
with three levels. It is sorted alphabetically, which for our purposes is okay
because we would like "normal", the first category, to be the **referent** group.

**Referent group**: The group that the others will be compared to. Here, we will
compare the osteopenia and osteoporosis groups to the normal group. Oftentimes,
we set the referent group to be the level with the relatively best health outcome
compared to the other groups. 

### Defining a new referent group

If you wanted to change the referent group, you could use `fct_relevel()`, which
we used in Part I of the course.

```{r}
calcium_data <- calcium_data %>% mutate(type_reordered = fct_relevel(type, "osteoporosis")) #makes osteoporosis the referent level.

levels(calcium_data$type_reordered)
```

* We will run two regressions, one with `type` and the other with `type_reordered` to see how their outputs differ.

### Linear regression when $x$ is categorical

Recall the form of the regression model when $x$ and $y$ are both continuous:

$$y = a + bx$$

We can write this more precisely. The predicted value for individual $i$ is represented by:

$$y_i = a + bx_i$$

Suppose you have a categorical variable with three levels. Then the new form of the regression model is:

$$y_i = a + b_1*[\text{i's category == osteopenia}] + b_2[\text{i's category == osteoporosis}]$$

### Linear regression when $x$ is categorical

General form of regression model for categorical $x$ variable with three levels:

$$y_i = a + b_1*[\text{i's category == osteopenia}] + b_2[\text{i's category == osteoporosis}]$$

* When an individual is in the normal bone density category, then both `category == osteopenia` and `category == osteoporosis`
are FALSE and the regression model simplifies to:

$$y_i = a$$

* When an individual is in the osteopenia bone density category, then `category == osteopenia` is TRUE and `category == osteoporosis`
is FALSE and the regression model simplifies to:

$$y_i = a + b_1$$

* When an individual is in the osteoporosis bone density category, then `category == osteopenia` is FALSE and `category == osteoporosis` is TRUE and the regression model simplifies to:

$$y_i = a + b_2$$

Thus, when $x$ is categorical, the regression model predicts each individual's measure to be at the mean for that category.

### Running `lm()` on categorical $x$ data

```{r}
library(broom)
calcium_lm <- lm(calcium_intake ~ type, data = calcium_data)

tidy(calcium_lm)
```

Interpretation of these data:

* The intercept is equal to 938.33. This is the average bone density for a person with normal density.
* The coefficient for "osteopenia" is equal to -138.33. This means that individual's with osteopenia had bone densities that are on average 138.33 lower than individuals with normal bone density.
* The coefficient for "osteoporosis" is equal to -223.33. This means that individual's with osteoporosis had bone densities that are on average 223.33 lower than individuals with normal bone density.

### Running lm() on categorical $x$ data

The coefficient estimates based on the model agree with what we calculate by hand:

```{r}
calcium_data %>% 
  group_by(type) %>% 
  summarise(mean = mean(calcium_intake))
```

* Note that 800 is 138.33 lower than 938.33
* Note that 715 is 223.33 lower that 938.33

### Tests and p-values based on the linear model

```{r}
library(broom)
calcium_lm <- lm(calcium_intake ~ type, data = calcium_data)

tidy(calcium_lm)
```

* We can also interpret these p-values. What is the null hypothesis?
* For the second row, the null hypothesis is that the regression coefficient for osteopenia is equal to 0. That is, the mean
for patients with osteopenia is equal to the mean for those with normal bone density. 
* The p-value is 0.32, so we do not reject the null hypothesis. 

### `predict` function to get 95% confidence intervals

We can use R code to calculate the 95% confidence intervals for the means:

```{r, warning=F}
newdata = data.frame(type=c("normal", "osteopenia", "osteoporosis")) #make a tiny data frame storing the three categorical levels
predictions <- tidy(predict(calcium_lm, newdata, interval="confidence")) #predict calcium intake for each row in newdata, i.e., for each level of the categorical variable
predictions$type <- c("normal", "osteopenia", "osteoporosis") #append another row to the data frame with the category labels
predictions
```

### Plot the observed data, the predicted means and their 95% CIs

What does this remind you of? It looks like the plots we made when we did ANOVA!

```{r}
ggplot(data = calcium_data, aes(x = type, y = calcium_intake)) + geom_point() +
  geom_point(data = predictions, aes(y = fit), col = "red") +
  geom_segment(data = predictions, aes(y = lwr, yend = upr, xend = type), col = "red") + 
  theme_minimal()
```

### Run the model again using the `type_reordered` as the exposure variable

* Recall that `type_reordered` contains the same factor variable but with osteoporosis as the reference group

```{r}
levels(calcium_data$type_reordered)
```

* Write the regression equation for the model with `type_reordered` as the 
`x` variable
* Before running the linear model, how do you expect the regression output to
change?

### Regression equation

$$y_i = a + b_1*[\text{i's category == normal}] + b_2[\text{i's category == osteopenia}]$$

Thus, $a$ (the intercept) will be the average for patients with osteoporosis,
$a + b_1$ will be the average for patients with normal bone density (implying
the b_1 will be the additional calcium intake for patients with normal bone density) 
and $a + b_2$ will be the average for patients with osteopenia (implying that
b_2 will be the additional calcium intake for patients with osteopenia)

### The model

```{r}
calcium_lm2 <- lm(calcium_intake ~ type_reordered, data = calcium_data)

tidy(calcium_lm2)
```

* Can you interpret the intercept and other coefficients in this model?
* What is the average calcium intake for someone with osteopenia? Is it lower or 
higher than someone with normal bone density? By how much?

### Another model: Mistaking categorical data for continuous data

* Recall that `type_num` is a numeric way of storing the categorical data stored
in `type`

```{r}
str(calcium_data$type_num)
```

* What happens if we run the regression on $type_num$?

### Another model: Mistaking categorical data for continuous data

Run the regression on `type_num`:

```{r}
calcium_lm3 <- lm(calcium_intake ~ type_num, data = calcium_data)

tidy(calcium_lm3)
```

* Notice that there is only one coefficient for type_num, but we were expecting
two coefficients -- one for the two non-referent levels of type.
* What happened?
* Answer: R interpreted `type_num` as a continuous, not categorical, variable.
It estimated a regression slope term using $y = a + bx$, which is not what we 
want. This linear model makes the assumption that the increase in calcium intake
going from category 1 to 2 and from 2 to 3 is the same. Thus, this model is not 
what we wanted to fit, and does not reflect the underlying data.
* Lesson: make sure that you double check how your categorical variables are coded!
They should be store as factors or else R will treat them as continuous variables.

### Changing a variable type from continuous to categorical

* If your factor variable was coded numerically (like `type_num` in this example),
R will code it as a continuous number and run simple linear regression on the underlying numbers. This is wrong, but can happen by mistake if you don't check.
* In this case you need to change the storage type using `your_data %>% mutate(var_categorical = as.factor(var_numeric))`
* In the code below, we update the storage type for `type_num` and store as a new categorical variable called `type_cat_ii`:

```{r mutate-numeric-to-categorical}
calcium_data <- calcium_data %>% mutate(type_cat_ii = as.factor(type_num))

str(calcium_data)
```

Re-run the model on `type_cat_ii`:

```{r}
calcium_lm4 <- lm(calcium_intake ~ type_cat_ii, data = calcium_data)

tidy(calcium_lm4)
```

* This looks better (we have two coefficients). 
* But it is confusing, we have to know which numbers correspond to which categories. Better to code the underlying data using words (like the original `type` variable)
to prevent mistakes and confusion.

### Recap

* We now know how to write linear models when $y$ is continuous and $x$ is either
continuous or categorical
* When $x$ is categorical we expect $k-1$ regression coefficients, where $k$ is the number. The regression coefficients correspond to the average value of $y$ for
each category.
* When we have categorical data, it is important to make sure R knows it is categorical (by checking its `str()`) and setting the appropriate referent group.
* Hypothesis tests as how much each group differs from the referent group.
of categories

### Future stats classes...

* We've only dealt with one $x$ variable at a time. In future stat courses, you 
will learn how to model multiple $x$ variables using multiple linear regression.
* This is important for prediction models (to get the best prediction you include
every $x$ variable that helps predict $y$)
* This is also important for causal models (for these models, you are interested in the causal effect of a specific $x$ variable, but include other $x$ variables to control for bias, such as confounding variables, and model interactions).