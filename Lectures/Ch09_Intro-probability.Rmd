---
title: "Chapter 9: Essential Probability Rules"
author: "Corinne Riddell"
date: "September 25, 2019"
output: slidy_presentation
---

### Learning objectives for today

* Why does probability matter?
* How to estimate a probability from a sample
* How do estimates change as the sample size increases?
* Learn new terminology: sample space (discrete vs. continuous), event, discrete probability model, continuous probability model
* Basic probability rules
* What is a random variable
* More definitions: risk, odds, case fatality rate

```{r load-libraries, message=F, warning=FALSE, echo=F}
library(ggplot2)
library(dplyr)
library(patchwork)
library(readxl)
library(ggrepel)
library(gridExtra)
library(cowplot)
```

### Why is probability important

- Statistics can be misleading, knowing the basic rules of probability helps you
interpret statistics more clearly.

### Three examples on the next slides show that

1) Misleading statistics can be used to make and defend policy decisions
    - You need to know how to interpret those statistics for yourself
2) Determining probability can be difficult and not intuitive. Our gut instinct about the probability of an event may be way off and lead to poor decision making in a medical setting
3) Using a predictive models that calculate a probability sounds robotic and unbiased. But these models encode bias and discrimination.

### Example. 1: Misleading statistics can be used to make and defend policy decisions

- Kirstjen Nielsen, former Homeland Security Secretary stated, about folks at the 
US-Mexico border:

> “Again, let’s just pause to think about this statistic: **314 percent** 
increase in adults showing up with kids that are not a family unit...Those are 
traffickers, those are smugglers, that is MS-13, those are criminals, those are 
abusers.”

- Nielsen was speaking about a relative increase in the probability of the event 
of "adults with kids who are not their own at the US-Mexico border". The 
relative increase is very large (314%). However, how often did the event happen 
in the first place?

- In a Washington Post analysis^1^, the increase was from 0.19% in
2017 to 0.61% in 2018. Thus the actual chance of the event happening is very 
small and increased by 0.61%-0.19% = 0.42 percentage points. 

- Takeaway: looking at the increase in absolute precentage points provides a 
different interpretation than the increase on the relative scale.

Reference: https://www.washingtonpost.com/news/politics/wp/2018/06/18/how-to-mislead-with-statistics-dhs-secretary-nielsen-edition/?noredirect=on&utm_term=.9193534ee80c

### Example 2: Calculating probabilities in medical settings can be difficult and not intuitive

- Suppose that there is test for a specific type of cancer that has a 90% chance 
of testing positive for cancer if the individual truly has cancer and a 90% chance of 
testing negative for cancer when the individual does not have it. 
- 1% of patients in the population have the cancer being tested for. 
- What is the chance that a patient has cancer given that they test positive? 

a) Between 0% - 24.9%
b) Between 25.0% - 49.9%
c) Between 50.0% - 74.9%
d) Between 75.0% - 100%

### Example 2: Calculating probabilities in medical settings can be difficult and not intuitive

Many people choose d), but the true answer is a)! Why do we get this so wrong?

Video link (2 mins): [click here](https://www.youtube.com/watch?v=nuSqWRuz-mU&feature=youtu.be) 

### Example 3: Algorithms can be biased and discriminatory

- Algorithms are increasingly used to automate decision-making. 
- One algorithm aids in decision-making by judges during criminal sentencing. Judges
want to determine who is likely to re-offend.
-  An investigative report by ProPublica^2^ found that: 

> "the formula was particularly likely to falsely flag black defendants as future criminals, 
wrongly labeling them this way at almost twice the rate as white defendants. 
White defendants were mislabeled as low risk more often than black defendants"

- Takeaway: algorithms to calculate probability can be biased and discriminatory.

Reference: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing

### Understanding probability

- These three examples illustrate the importance of understanding probability
in medicine, public policy, and decision-making
- Often, our understanding of probability is based on estimates from a sample drawn from a probability
- Last Friday, we talked about how larger samples provide better estimates on average.
- The following slides review this concept (for you to read on your own time)

### From B&M page 216: How common is the common cold?

- Suppose there are 100,000 people in your community. You work for your 
community's public health office and want to estimate the number of people who 
had a common cold. You are not able to sample everyone but suppose you could 
randomly call people in your community and ask them "Did you have a cold 
yesterday?" and then calculate the proportion of the sample who had a cold. 

### From B&M page 216: How common is the common cold?

- Suppose you somehow had access to data on the whole population in a data frame.
- For each person, the variable `had_cold_yesterday` equals 0 if they did not 
have a cold yesterday, and 1 if they did have a cold.
- Suppose these data are called `cold_data` with the following outputs:

```{r create-cold-data, echo=F}
#students: you don't need to understand this code chunk yet. 

population_size <- 100000
prob_cold <- 0.112
id <- 1:population_size
set.seed(1234)
# Generate n random binomial samples (coin flips) where 
# probability of cold (heads) is prob_cold
had_cold_yesterday <- rbinom(n = population_size, size = 1, prob = prob_cold) 
#each person has an 11.2% chance of having the common cold yesterday
cold_data <- data.frame(id, had_cold_yesterday)
```

```{r}
dim(cold_data)
head(cold_data, 20)
cold_data %>% summarize(population_mean = mean(had_cold_yesterday))
```

- The average (mean) of a variable containing only 0's and 1's is called a 
**proportion**. This is because the mean is the number of individuals with a 
cold (coded as `had_cold_yesterday = 1`) divided by the total number of individuals.

### From B&M page 216: How common is the common cold?

Realistically, you would not have access to data on every person in the population.
You need to take a sample instead. How many people should be included in the sample?

### From B&M page 216: How common is the common cold?

Realistically, you would not have access to data on every person in the population.
You need to take a sample instead. How many people should be included in the sample?

- We want to sample enough people such that the proportion of those with colds
in the sample is close to the proportion of those with colds in the population

- Let's take samples of size 5, 100, 1000, and so on, using `dplyr`'s `sample_n`
function:

```{r the-smallest-samples}
sample_5 <- dplyr::sample_n(tbl = cold_data, size = 5)
sample_100 <- dplyr::sample_n(tbl = cold_data, size = 100)
sample_1000 <- dplyr::sample_n(tbl = cold_data, size = 1000)
sample_10000 <- dplyr::sample_n(tbl = cold_data, size = 10000)
sample_100000 <- dplyr::sample_n(tbl = cold_data, size = 100000)
```

### Proportion with cold as a function of sample size

```{r results-from-the-smallest-samples}
sample_5 %>% summarize(sample_mean_n5 = mean(had_cold_yesterday))
sample_100 %>% summarize(sample_mean_n100 = mean(had_cold_yesterday))
sample_1000 %>% summarize(sample_mean_n1k = mean(had_cold_yesterday))
sample_10000 %>% summarize(sample_mean_n10k = mean(had_cold_yesterday))
sample_100000 %>% summarize(sample_mean_n100k = mean(had_cold_yesterday))
```

- What do you notice about the proportion estimates? 
- Do they approach the true estimate as the sample size increases?

### How many people should we sample?

- So we know we should sample more than five people, but feasibly can't sample 
everyone. 

- We need to sample *enough* people to reasonably estimate the true chance of 
having a cold. But how many is enough?

### Proportion with cold as a function of sample size

* To help us decide I first sampled one person and took the mean of that sample.

* Then I added another person and took the mean of that sample of size 2 (n=2) 
and so on, until I had 5000 people. 

* The plot on the next slide shows the estimated proportion vs. the sample size.

```{r three-growing-samples-of-5000, echo=F, eval=F}
# students, you don't need to understand the code in this chunk

cold_data_copy <- cold_data
cold_data_copy$in_sample1 <- 0
cold_data_copy$in_sample2 <- 0
cold_data_copy$in_sample3 <- 0
grow_data1 <- grow_data2 <- grow_data3 <- NULL

for(i in 1:5000){
  sample_1 <- cold_data_copy %>% 
    filter(in_sample1 == 0) %>% 
    sample_n(1)
  cold_data_copy$in_sample1[cold_data_copy$id == sample_1$id] <- 1
  new <- cold_data_copy %>% filter(in_sample1 == 1) %>% 
    summarize(mean = mean(had_cold_yesterday),
              sample_size = n())
  grow_data1 <- rbind(grow_data1, new)
  
  sample_2 <- cold_data_copy %>% 
    filter(in_sample2 == 0) %>% 
    sample_n(1)
  cold_data_copy$in_sample2[cold_data_copy$id == sample_2$id] <- 1
  new <- cold_data_copy %>% 
    filter(in_sample2 == 1) %>% 
    summarize(mean = mean(had_cold_yesterday),
              sample_size = n())
  grow_data2 <- rbind(grow_data2, new)
  
  sample_3 <- cold_data_copy %>% 
    filter(in_sample3 == 0) %>% 
    sample_n(1)
  cold_data_copy$in_sample3[cold_data_copy$id == sample_3$id] <- 1
  new <- cold_data_copy %>% 
    filter(in_sample3 == 1) %>% 
    summarize(mean = mean(had_cold_yesterday),
              sample_size = n())
  grow_data3 <- rbind(grow_data3, new)
}

grow_data1 <- grow_data1 %>% mutate(sample = "First sample")
grow_data2 <- grow_data2 %>% mutate(sample = "Second sample")
grow_data3 <- grow_data3 %>% mutate(sample = "Third sample")

grow_data <- rbind(grow_data1, grow_data2, grow_data3)
## Saving image because this part takes a long time
save.image("../Data/chp9.rdata")
```

### Estimated proportion vs. sample size for n = 250

```{r first-sample-250, echo=F,fig.align='center', message=F, warning=F, out.width = "80%"}
# students, you don't need to understand the code in this chunk

load("../Data/chp9.rdata")
segment1 <- data.frame(x1 = 1, x2 = 1000, y1 = prob_cold, y2 = prob_cold)
segment2 <- data.frame(x1 = 1, x2 = 1000, y1 = prob_cold + 0.05, y2 = prob_cold + 0.05)
segment3 <- data.frame(x1 = 1, x2 = 1000, y1 = prob_cold - 0.05, y2 = prob_cold - 0.05)
segment4 <- data.frame(x1 = 1, x2 = 1000, y1 = prob_cold + 0.02, y2 = prob_cold + 0.02)
segment5 <- data.frame(x1 = 1, x2 = 1000, y1 = prob_cold - 0.02, y2 = prob_cold - 0.02)

p1 <- ggplot(grow_data %>% dplyr::filter(sample_size < 250, sample == "First sample"), 
             aes(x = sample_size, y = mean)) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment1) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment2, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment3, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment4, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment5, lty = 3) + 
  geom_text(aes(y = prob_cold + 0.005, x = 800), label = "pop'n proportion", check_overlap = T) +
  geom_text(aes(y = prob_cold + 0.05, x = 900), label = "+ 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.05, x = 900), label = "- 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold + 0.02, x = 900), label = "+ 2", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.02, x = 900), label = "- 2", check_overlap = T) + 
  geom_line(aes(col = factor(sample))) + 
  labs(y = "Estimated proportion", x = "Sample size", 
       title = "n=250, one sample") +
  theme_minimal(base_size = 15) + 
  scale_x_continuous(breaks = seq(0, 1000, 200)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 0.18, 0.02), 
                     limits = c(0, 0.18)) + 
  theme(panel.grid.minor = element_blank(), legend.position = "top", legend.title = element_blank())

# Now we add the line for the second sample of 250:
p2 <- ggplot(grow_data %>% dplyr::filter(sample_size < 250, sample != "Third sample"), 
             aes(x = sample_size, y = mean)) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment1) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment2, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment3, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment4, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment5, lty = 3) + 
  geom_text(aes(y = prob_cold+0.005, x = 800), label = "pop'n proportion", check_overlap = T) +
  geom_text(aes(y = prob_cold + 0.05, x = 900), label = "+ 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.05, x = 900), label = "- 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold + 0.02, x = 900), label = "+ 2", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.02, x = 900), label = "- 2", check_overlap = T) + 
  geom_line(aes(col = factor(sample))) + 
  labs(y = "Estimated proportion", x = "Sample size",
       title = "n=250, two samples") +
  theme_minimal(base_size = 15) +
  scale_x_continuous(breaks = seq(0, 1000, 200)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 0.18, 0.02), 
                     limits = c(0, 0.18)) +
  theme(panel.grid.minor = element_blank(), legend.position = "top", legend.title = element_blank()) 
  
p1 + p2 + plot_layout()  
```

### Estimated proportion vs. sample size for n = 250 and n = 500 

- Increase the sample size how the estimate becomes closer to the true value
- Add in a third sample to compare how different samples perform in the short 
vs. the long run 

```{r sample-250-500, echo=F, fig.align='center', message=F, warning=F, out.width="80%"}
p3 <- ggplot(grow_data %>% dplyr::filter(sample_size < 250), aes(x = sample_size, y = mean)) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment1) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment2, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment3, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment4, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment5, lty = 3) + 
    geom_text(aes(y = prob_cold+0.005, x = 800), label = "pop'n proportion", check_overlap = T) +
  geom_text(aes(y = prob_cold + 0.05, x = 900), label = "+ 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.05, x = 900), label = "- 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold + 0.02, x = 900), label = "+ 2", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.02, x = 900), label = "- 2", check_overlap = T) + 
  geom_line(aes(col = factor(sample))) + 
  labs(y = "Estimated proportion", x = "Sample size", 
       title = "n = 250, 3 samples") + 
  scale_x_continuous(breaks = seq(0, 1000, 200)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 0.80, 0.02)) +
  theme_minimal(base_size = 15) +
  theme(legend.position="none", panel.grid.minor = element_blank())

p4 <- ggplot(grow_data %>% dplyr::filter(sample_size < 500), aes(x = sample_size, y = mean)) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment1) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment2, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment3, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment4, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment5, lty = 3) + 
    geom_text(aes(y = prob_cold+0.005, x = 800), label = "pop'n proportion", check_overlap = T) +
  geom_text(aes(y = prob_cold + 0.05, x = 900), label = "+ 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.05, x = 900), label = "- 5", check_overlap = T) + 
  geom_text(aes(y = prob_cold + 0.02, x = 900), label = "+ 2", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.02, x = 900), label = "- 2", check_overlap = T) + 
  geom_line(aes(col = factor(sample))) + 
  labs(x = "Sample size", y = "Estimated proportion", 
       title = "n = 500, 3 samples") + 
  scale_x_continuous(breaks = seq(0, 1000, 200)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 0.80, 0.02)) +
  theme_minimal(base_size = 15) +
  theme(legend.position="none", panel.grid.minor = element_blank())

segment6 <- data.frame(x1 = 1, x2 = 5000, y1 = prob_cold, y2 = prob_cold)
segment7 <- data.frame(x1 = 1, x2 = 5000, y1 = prob_cold + 0.05, y2 = prob_cold + 0.05)
segment8 <- data.frame(x1 = 1, x2 = 5000, y1 = prob_cold - 0.05, y2 = prob_cold - 0.05)
segment9 <- data.frame(x1 = 1, x2 = 5000, y1 = prob_cold + 0.02, y2 = prob_cold + 0.02)
segment10 <- data.frame(x1 = 1, x2 = 5000, y1 = prob_cold - 0.02, y2 = prob_cold - 0.02)

p3 + p4 + plot_layout()
```

### Estimated proportion vs. sample size for n = 5000

```{r sample-5000, echo=F, fig.align='center', message=F, warning=F, out.width="80%"}
p5 <- ggplot(grow_data %>% dplyr::filter(sample_size < 5000), aes(x = sample_size, y = mean)) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment6) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment7, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment8, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment9, lty = 3) + 
  geom_segment(aes(y = y1, yend = y2, x = x1, xend = x2), data = segment10, lty = 3) + 
  geom_text(aes(y = prob_cold+0.005, x = 5050), label = "pop'n proportion", check_overlap = T) +
  geom_text(aes(y = prob_cold + 0.05, x = 4200), label = "+5", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.05, x = 4200), label = "-5", check_overlap = T) + 
  geom_text(aes(y = prob_cold + 0.02, x = 4200), label = "+2", check_overlap = T) + 
  geom_text(aes(y = prob_cold - 0.02, x = 4200), label = "-2", check_overlap = T) + 
  geom_line(aes(col = factor(sample))) + 
  labs(y = "Estimated proportion", x = "Sample size", 
       title = "n = 5000, 3 samples") + 
  theme_minimal(base_size = 15) +
  scale_x_continuous(breaks = seq(0, 5000, 500)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 0.80, 0.02))  +  
  theme(panel.grid.minor = element_blank(), legend.position = "top", legend.title = element_blank())

p5
```

### Summary of the example on estimating the proportion with a cold

- As the sample size increases, the estimated proportion becomes closer to the 
true proportion
- Random samples of the same size will provide different estimates of the true 
proportion, but will be closer to each other (and the true value) if they are 
"large enough"

# Some vocabulary, definitions, and rules

### Sample space

A **sample space** is the set of all possible outcomes. It is denoted by the letter $S$.

### Discrete sample space

- For example, the sample space for marital status: S = {Single, married, divorced, widowed}
- Discrete spaces remind us of nominal, ordinal, or discrete variables
- Anything that is countable with "gaps" between the events
- The notation is important: S = {elements in the space}

### Continuous sample space

- For example, The interval [0, 1]: S = {all numbers between 0 and 1}
- Continuous sample spaces remind us of continuous variables only
- The events are not countable (i.e., we cannot list the numbers between 0 and 1; they are infinite)

### Event 

- An event is one possible outcome or a set of outcomes
- For the discrete example, the event could be being divorced. Or it could be being divorced or being widowed.
- For the continuous example, the event could be a value between 0.4 and 0.5.
- Question: Why do we determine the probability of these events?
- Answer: We need to know the probabilities associated with the sample space.

### Proability model

- A probability model is a description of random phenomonema. 
- It consists of defining the sample space $S$ and the probability associated with
different events.
- In a few slides we will discuss **discrete** probability models and **continuous**
probability models.

### Rules of probability

1. Probabilities are numbers between 0 and 1. 
    * $0 \leq P(A) \leq 1$ 
2. All possible outcomes of a sample space ($S$) together have a probability of 1. 
    * $P(S) = 1$
3. If two events have a joint probability of 0 (i.e., no overlap in their event 
spaces) then they are **disjoint** and the probability of either event occurring
is the summation of their individual probabilities. 
    * $P(A or B) = P(A) + P(B)$, if A and B are disjoint events.
4. The probability of an event not occurring is 1 minus the probability of the 
event occurring. This event is called the **complement**. 
    * P(A does not occur) = 1 - P(A). 
    * Shorthand: $P(A') = 1 - P(A)$ or $P(A^c) = 1 - P(A)$

### Discrete probability model

- A probability model with a sample space made up of a list of individual outcomes
is called discrete
- To assign probabilities in a discrete model, list the probabilities of all the
individual outcomes. These probabilities must be numbers between 0 and 1 and must
sum to 1. The probability of any event is the sum of the probabilities of the
outcomes making up the event.

### Discrete probability model example

For example, we could survey a sample of people and ask them their marital status. 
Based on this survey we can calculate the portion of each event in the sample space:

| Single | Married | Divorced | Widowed |
|:------:|:-------:|:--------:|:-------:|
| 47%    | 30%     | 18%      | 5%      |

This is a discrete probabiltiy model shown in a table. How else could you display these data?

### Continuous probability model

- A continuous probability model assigns probabilities as areas under a **density**
curve. The area under the curve and between a range of specified values on the horizontal 
axis is the probability of an outcome in that range.
- What is a density curve? 

### Density curves

- Density curves are also known as **probabilty density functions**
- You can think of density curves as **smoothed histograms**. 

### Density curves using `geom_density()`

- Recall the data on cesarean delivery rates across hospitals in the US. We can 
use these data to also make a density plot (also called density curve)

```{r cs-variation-plot-example, echo = F, warning=F, message=F, out.width="80%", out.height = "50%"}
CS_dat <- read_xlsx("../Data/Ch02_Kozhimannil_Ex_Cesarean.xlsx", sheet = 1)

CS_dat <- CS_dat %>% 
  dplyr::select(Births, `Cesarean rate *100`) %>%
  dplyr::rename(num_births = Births, cs_rate = `Cesarean rate *100`)

CS_dat <- CS_dat %>% dplyr::mutate(cs_rate_binned = cut(cs_rate, 32))

## Basic histogram from cesarean delivery. Each bin is 2 wide.
basic.hist <- ggplot(CS_dat, aes(x=cs_rate)) + 
  geom_histogram(binwidth = 2) +
  ggtitle("Basic histogram") + 
  theme_minimal(base_size = 15)

# Draw with black outline, white fill
better.hist <- ggplot(CS_dat, aes(x=cs_rate)) + 
  geom_histogram(binwidth = 2, colour="black", fill="white") + 
  ggtitle("Histogram") + 
  theme_minimal(base_size = 15)

# Density curve (students: be able to make density plots like this)
basic.density <- ggplot(CS_dat, aes(x=cs_rate)) + 
  geom_density() + 
  labs(title = "Basic density") + 
  theme_minimal(base_size = 15)

# Histogram overlaid with density curve
# overlaying density plots onto histograms is a bit tricky,
# so don't worry about learning this bit of code.
density.hist <- ggplot(CS_dat, aes(x=cs_rate)) + 
  geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=2,
                   colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +  # Overlay with transparent density plot
  ggtitle("Histogram and density") + 
  theme_minimal(base_size = 15)

better.hist + density.hist + plot_layout()

# plotting formatting ideas from : http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/

# this summary dataset shows the relationship between the proportion in each bin and the density
# since density is an area, you need to take the proportion (height) and divide by the bin width
# (here it is 2.02 based on how we chose our bins) to get the density.
CS.summary <- CS_dat %>% 
  group_by(cs_rate_binned) %>% 
  summarize(n = n()) %>%
  mutate(prop = round((n/sum(n))*100,2), 
         density = (n/sum(n))/2.02) 
```

### Density curves

- From this plot, we can see that the density curve approximates the shape of 
the histogram very well. 
- Remember because there are infinitely many cesarean delivery rates that could 
be observed between 0 and 1 it is impossible to assign a finite probability to
any specific number. 
- If we did, we could do this infinitely and their summed probability would 
surpass 100%. 
- Instead, the density curve is used to determine the probability of an observed 
event being between a range of values.

### Density curves

- Define $CS$ to denote a cesarean delivery rate at a specific hospital. You 
could use the density curve to calculate:

- The probability that the CS rate is less than 20%, or $P(CS < 0.20)$
- The probability that the CS rate is between 20% and 40%, or $P(0.20 < CS < 0.40)$
- The probability the the CS rate is less than 20% or greater than 40%, or 
$P(CS < 0.2$ or $CS > 0.4)$. Since these ranges do not overlap (they are disjoint), 
we can rewrite this as $P(CS < 0.2) + P(CS > 0.4)$ 
- The probability that the CS rate is larger than 40% is written as $P(CS > 0.4)$. We can also write it as $1 - P(CS \leq 0.4)$ by applying the complement rule.

### Interpretations of probability

- The calculations on the previous slide can be interpreted as either:
    - the **probability** that a randomly chosen hospital will have cesarean delivery
    rate in the specified range.
    - the **proportion** of hospitals with cesarean delivery rates in the specified 
    range
- Thus, the interpretation can both be applied to an individual or a population.

### Random variables

- A random variable is a variable whose value is a numerical outcome of a random 
phenomenon
- Random variables are represented by capital letters, most popularly the letter X. 
- A lower case letter represents a particular value for the random variable has 
been taken. For example $P(X=x)$ asks, what is the probability that random 
variable $X$ takes the value $x$? 
- For continuous random variables, we only calculate the probability of $X$ falling
in a specified range, e.g., $P(X < x)$, $P(X \geq x)$, or $P(x_1 < X < x_2)$
- Remember: for continuous random variables $P(X=x)=0$!

### The mean and standard deviation revisted

- In Chapter 2, we learned about how to calculate the mean ($\bar{x}$) and 
standard deviation ($s$) of a sample. 
- We can also calculate the mean ($\mu$) and standard deviation ($\sigma$) of a 
population. 
- The mean and standard deviation of a population are represented using different
notation to remind us that we are describing a population *parameter* vs. the *sample* 
mean and *sample* standard deviation that are *statistics* used to describe samples. 

### Putting it all together 

The figure illustrates the difference between a (hypothetical) underlying 
distribution of heights among women in the US and its mean and s.d. vs. that of 
the sampled distribution. 

```{r, fig.align='center', out.width="80%", echo=F}
knitr::include_graphics("../Images/Ch09_Sampling-and-inference.png")
```


### Risk

- Generally speaking, **risk** is another word for the probability or chance of
an event occurring. In epidemiology and public health, we often use the word risk
to represent the risk of some adverse health outcome among a group of individuals. 

- Here is the probability of dying from a specific cause given that a death from
one of the top causes has occurred

- Notice that even among the top 10 causes, there is a wide range in the chance of
occurrence

```{r prob-spectrum-deaths, echo=F, out.width="95%"}
top.causes <- read_xlsx("../Data/Ch09_Top-causes-of-death.xlsx", sheet = 1)
segment <- data.frame(x1 = 0, x2 = 25, y1 = 1, y2 = 1)

ggplot(top.causes, aes(x = Percent.of.deaths, y = 1)) + 
  geom_segment(aes(x = x1, xend = x2, y = y1, yend = y2), data = segment, col = "grey") +
  geom_point(aes(col = factor(Rank)), size = 2) + 
  geom_text_repel(aes(label = paste(Short.title, Percent.of.deaths, "%"), col = factor(Rank))) +
  scale_x_continuous(limits = c(0, 26)) +
  theme_minimal() + 
  scale_y_continuous(limits = c(0.8, 1.2), breaks = NULL) +
  ylab("") + xlab("Probability") +
  guides(col = F) + ggtitle("Percent of deaths from top causes") + 
  labs(caption = "Data from https://www.cdc.gov/nchs/data/dvs/LCWK9_2015.pdf")
```

### Other risk definitions

- **Attack rate** of a virus: the risk (probability) of becoming afflicted during an
infectious period, like the flu season. If the attack rate of influenza during a 
specific flu season was 10% or 10 per 100, this would imply that 10 out of every 
100 individuals develop influenza during the epidemic period. 

- **Case fatality rate**: the risk (probability) of dying among individuals with a 
specified condition. For example, the case fatality rate of measles is 1.5 per 
1000 cases or 0.15%. (Examples from Epidemiology an Introduction by KJ Rothman, 
2002:p.28)

- Note that these definitions use the word "rate" but not in the same way that we
define rates in epidemiology. The attack rate and case fatality rate are risks, 
not rates!

### Odds

- The **odds** is another commonly used measure in epidemiology that is a ratio of 
the probability of the adverse event over the probability of adverse event not 
occurring. That is, if the risk of event A is equal to $p$ then the odds of the
event is equal to $\frac{p}{1-p}$
- If both parents have the gene for sickle cell anemia, the chance that their biologic child will have the disease is 25%. The odds their child will have the disease is $\frac{0.25}{1-0.25}=0.33$ or 1:3.
- Sometimes, the popular press uses the word "odds" when we would use the word risk
or probability. 

### Comparing risks and odds between populations or groups

- In public health and epidemiology, we often contrast the risks and odds of
a health outcome between two groups of individuals to ask: Is the risk of the 
outcome lower (or higher) in individuals who are exposed vs. not exposed (or 
treated with drug A vs. drug B)?

### Recap: What new concepts did we learn?

- Many definitions: Event, random variable, sample space (discrete and continuous),
probability model (discrete and continuous), rules of probability, density curves, risk, odds