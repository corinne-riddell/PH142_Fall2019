---
title: "Chapter 15: Inference in Practice"
author: "Corinne Riddell"
date: "October 21, 2019"
output: pdf_document
---

### Recall the simple conditions for inference

1. SRS
2. Population has a Normal distribution
3. $\sigma$ is known

### Condition 1: Where do the data come from?

When you use statistical inference, you are acting as if your data are a 
SRS or come from a randomized experiment.

Sometimes we treat non-random samples as if they were random. We can do this if 
we believe that the non-randomness of the sample does not affect the outcome of 
interest

### Condition 1: Where do the data come from?

* A neurobiologist is interested in how visual perception can be fooled
by optical illusions. 
* A sociologist is interested in attitudes toward the use of human subjects in 
science.
* They both use their students as convenience samples. Can we act as if these 
students are a SRS? 
* It is easier to make this argument for one of these research questions. Which 
one? And why?

### Condition 1: Where do the data come from?

Even if the data come from a randomized experiment, there could be issues that
hinder the randomness:

- Non response
- Dropout ("Lost to follow-up", in RCT jargon)

### Condition 2: What is the shape of the population distribution?

- There is flexibility with this condition. We started by assuming Normality of 
the distribution, which guarantees that the sampling distribution for the mean 
is Normally distributed, no matter the sample size.
- However, because of the Central Limit Theorem, we know that the sampling 
distribution for the mean where the data come from any underlying distribution 
will eventually become Normally distributed when the sample size $n$ is large 
enough.
- The z-test is reasonably accurate for any symmetric distribution if the sample
size is moderate
- If the distribution is skewed, need a large enough $n$ for the z-test to work.

### Condition 2: What is the shape of the population distribution?

- Examine the shape of the sample's distribution using a Normal Quantile plot 
(ideally) or a histogram. Use it to infer the shape of the population distribution
- Difficult to infer much if there are too few observations.

### Condition 2: What is the shape of the population distribution?

- Outliers can affect tests of non-resistant measures like the mean. 
- Double check if the outlier is "real" or an error. If it is real, can use other 
methods that aren't sensitive to outliers.

### How confidence intervals behave

Recall the form of a CI: 

$$\bar{x} \pm z^*\frac{\sigma}{\sqrt{n}}$$

Where $z^*\frac{\sigma}{\sqrt{n}}$ is the **margin of error**.

The margin of error gets smaller when: 

- z* is smaller (i.e., you change to a lower confidence level). To obtain a 
narrower confidence interval using the same data (i.e., same $\sigma$ and $n$)
you need to accept a lower confidence level, implying a trade-off between the 
confidence level and the margin of error.
- $\sigma$ is smaller. You might be able to reduce $\sigma$ if there is measurement
error of the variable that you can improve or eliminate. Often times, the 
$\sigma$ can't be reduced, it is just a characteristic of the population.
- $n$ is larger. You need to quadruple the sample size to halve the margin of 
error:
    - Solve this equation for m: $z^*\frac{\sigma}{\sqrt{n}} = \frac{1}{2}z^*\frac{\sigma}{\sqrt{m}}$ and you will get $m=4n$

### The margin of error only accounts for sampling error

- This is one of the most important points!
- If you're taking epidemiology, you've likely learned about epidemiologic bias:
confounding, measurement error, and selection bias. These are **systematic errors**.
The confidence interval does not account for systematic errors.

### How hypothesis tests behave

- The p-value of a test is dependent on whether $H_a$ is one-sided or two-sided.
- The p-value for a two-sided test is double the p-value for the one-sided test
of the same $H_0$
- It is important to determine whether you are conducting a one- or two-sided
test before you see the data, and if one-sided, whether you are testing if 
$\mu < \mu_o$ or $\mu > \mu_0$. That is you should never use $\bar{x}$ as a 
basis for determining the direction of the alternative hypothesis.

### How hypothesis tests behave

- Statistical significance depends on sample size (since sample size determines
the standard error of the sampling mean)

- Recall the form of the z-test:

$$z = \frac{\bar{x}-\mu}{\sigma/\sqrt{n}}=\frac{\text{magnitude of observed effect}}{\text{size of chance variation}}=\frac{signal}{noise}$$

- The numerator quantifies the distance between what you observe in the sample 
and the null hypothesized parameter.
- The denominator represents the size of chance variations from sample to sample
- Statistical significance depends on:
    - The size of the observed effect ($\bar{x}-\mu$)
    - The variability of individuals in the population ($\sigma$)
    - The sample size ($n$)

### How hypothesis tests behave

- Statistical significance depends on:
    - The size of the observed effect ($\bar{x}-\mu$)
    - The variability of individuals in the population ($\sigma$)
    - The sample size ($n$)
- If you obtain a small p-value it is not necessarily because the effect size is large.
- Very tiny effects can be deemed statistically significant when you have enough
data. This is a big problem in the age of big data, because you're almost 
guaranteed to obtain statistically significant results, no matter the effect size.
- On the other hand, if your sample size is too small, you might not obtain statistical
significance even if your effect size is large. Thus "an absence of evidence is 
not evidence of absence", or failing to reject the null hypothesis does not imply
that the null hypothesis is true.
    
### How hypothesis tests behave

- Statistical significance is different from clinical/practical significance. 
- A statistically significant result might not be large enough to be important in 
context.

### P-values 

- Watch this 11-min YouTube video on “P-hacking”: https://www.youtube.com/watch?v=Gx0fAjNHb1M
- Read this Vox article about a Cornell food researcher how engaged in p-hacking: https://www.vox.com/science-and-health/2018/9/19/17879102/brian-wansink-cornell-food-brand-lab-retractions-jama
- Read this two-page ASA brief on statistical significance and p-values: https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf
- We don't have time to discuss these today, but the material contained in these
sources is testable!

### Selecting an appropriate sample size 

How big does the sample size need to be?

Suppose you want your margin of error to equal $m$. What sample size do you need
to obtain a margin of error of $m$? 

You can rearrange the formula for the margin of error (moe) to solve for $n$:

$$moe=\frac{z^*\sigma}{\sqrt{n}}$$
$$n=\Big(\frac{z^*\sigma}{moe}\Big)^2$$

### Example of calculation sample size

Body temperature has a known $\sigma=0.6$ degrees F. We want to estimate the mean
body temperature $\mu$ for healthy adults within $\pm0.05$ F with 95% confidence. 
How many healthy adults must we measure?

$$n=\Big(\frac{z^*\sigma}{moe}\Big)^2$$
$$n=\Big(\frac{1.96\times 0.06}{0.05}\Big)^2 = 553.2$$

* We must recruit 554 healthy adults for this study.
* Note that we always round up when calculating sample size, because if we rounded
to the nearest whole digit (553) then the margin or error would be smaller than 
$\pm{5}$.

### Sample size when conducting a hypothesis test

To think about sample size for a z-test, three things matter:

- **Significance level:** How much protection do we want against saying there is
"evidence against $H_0$ in favor of $H_A$" when there really is no effect
in the population? The significance level we choose fixes this conditional 
probability to a fixed level, often 5%: $P(\text{Reject } H_0|H_0 \text{ is true}])= 0.05$
- **Effect size:** How large of an effect in the population is important in practice?
- **Power:** How confident do we want to be that our study will detect an effect
of the size we think is important? I.e., what is the probability of rejecting
$H_0$ when the alternative hypothesis is true? That is $P(\text{Reject }H_0|H_A \text{ is true})$

### Example of calculating power

* Suppose you know that $\sigma = 1$. 
* $H_0: \mu = 0$ 
* $H_a: \mu > 0$ 
* Set $\alpha = 0.05$. 
* Additional condition: You need to be 90% confidence that the test will reject
$H_0$ when the true underlying mean equals $\mu=0.8$. That is, you want 90% 
**power** when $\mu=0.8$.

### Example of calculating power

First, calculate the minimum z-value required to reject $H_0$:

```{r}
qnorm(p = 0.05, mean = 0, sd = 1/sqrt(10), lower.tail = F)
```

So for any z-test with this value or higher, you will reject $H_0$ in favor of $H_a$.

Now suppose that $\mu=0.8$ is true. The test will reject $H_0$ about what 
percent of the time when $H_a$ is true? To calculate this probability, we take 
the value from the previous calculation and calculate the *probability* above 
its value under $H_A$:

```{r}
pnorm(q = 0.5201484, mean = 0.8, 1/sqrt(10), lower.tail = F)
```

Thus, you have a 82% chance of obtaining evidence in favor of $H_A$ when $\mu=0.8$
if $n=10$. To obtain power of 90%, you will need to increase the sample size.

### Example of calculating power, illustrated

```{r, echo= F, fig.align='center', out.width="80%"}
library(ggplot2)
ggplot(data = data.frame(x = c(-1.3, 2)), aes(x)) + 
    geom_area(stat = "function", fun = dnorm, args = list(mean = 0, sd = 1/sqrt(10)), 
            fill = "red", xlim = c(0.5201484, 1.1)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1/sqrt(10))) +
  labs(y = "density") + 
  theme_minimal(base_size = 15) +
  labs(title = "Calculating power") +
  geom_area(stat = "function", fun = dnorm, args = list(mean = 0.8, sd = 1/sqrt(10)), 
            fill = "blue", xlim = c(0.5201484, 2.0), alpha = 0.5) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0.8, sd = 1/sqrt(10))) +
  geom_text(aes(x = 0, y = 1.2), label = "H0", check_overlap = T) +
  geom_text(aes(x = 0.8, y = 1.2), label = "Ha", check_overlap = T)

```

* There is 5% of the area under $H_0$. This is the chance of rejecting the null
hypothesis even when it is true. We found that x = 0.5201484 denotes the value
where an $\bar{x}>=0.5201484$ would provide evidence against $H_0$ at 
$\alpha = 0.05$
* Then, we sketch the distribution of the alternative if $\mu=0.8$ and we calculate
the probability of rejecting $H_0$ when $\mu=0.8$ is true.

### Power

Thus, for this example, if you want power of 90%, you need to increase $n>10$.

Based on the calculations below, $n=14$ reaches 90% power: 

```{r}
n <- 13
z <- qnorm(p = 0.05, mean = 0, sd = 1/sqrt(n), lower.tail = F)
pnorm(q = z, mean = 0.8, sd = 1/sqrt(n), lower.tail = F)

n <- 14
z <- qnorm(p = 0.05, mean = 0, sd = 1/sqrt(n), lower.tail = F)
pnorm(q = z, mean = 0.8, sd = 1/sqrt(n), lower.tail = F)
```

### What affects the sample size required for a hypothesis test?

- Smaller significance levels require larger $n$
- Higher power requires a larger $n$
- For a given $\alpha$ and power, two-sided alternatives require a larger $n$ than one-sided tests
- Aiming to detect a smaller difference between $\bar{x}$ and $\mu$ requires
a larger $n$

### Power, Type I error, and Type II error in hypothesis tests

- The significance level $\alpha$ is the chance of making a wrong decision  when
the null hypothesis is true. This is known as a **type I error**
- The power is the chance of making the right decision when the alternative is true.
    - Thus, its complement, 1-power, is the chance of making a wrong decision 
    when the alternative hypothesis is true. This is known as a **type II error.**
    - We let $\beta$ denote the chance of a Type II error. $\text{Power}=1-\beta$

### Power, Type I error, and Type II error in hypothesis tests

|                      | $H_0$ is true                 | $H_a$ is true                 |
|----------------------|:-----------------------------:|:-----------------------------:|
| Reject $H_0$         | Type I error ($prob = \alpha$)| Correct decision              |
| Fail to reject $H_0$ | Correct decision              | Type II error ($prob = \beta$)|

