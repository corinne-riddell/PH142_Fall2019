---
title: 'Chapter 4: Introduction to Regression'
author: "Corinne Riddell"
date: "September 13, 2019"
output: slidy_presentation
---

### Learning objectives for today

1) Introduction to linear regresssion
    - How do we find the line of best fit?
    - What is its slope?
    - What is its intercept?
    - What is the R-squared?

2) Use R to run a linear regression and add a regression line to a scatter plot

3) Learn how to transform non-linear data so that we can use linear regression

4) Learn how outliers influence the line of best fit

5) Understand why: 
    - Association does not necessarily mean causation
    - We should not extrapolate beyond our data
    - We should always consider potential **confounders** in our interpretation
    - We should use data visualization to confirm the shape of the relationship
    
### What is a regression line?

- A straight line that is **fitted** to data to minimize the distance between 
the data and the fitted line. 
- It is often called the **line of best fit**. 
- It is also called the **least-squares regression line **  (sometimes refered
to as **ordinary least squares** or **OLS**)  this is because, mathematically, 
the criteria for choosing this line is based on the sum of squares of the 
vertical distances from the line.  We choose the line that minimizes this sum.

### What is a regression line?

Once we calculate this line, it can be used to describe the relationship between
the explanatory and response variables.

- *Can* you fit a line of best fit for non-linear relationships? *Should* you?
- Very important to visualize the relationship first. Why?

### Equation of the line of best fit

The line of best fit can be represented by the equation for a line:

$$y = a + bx$$

where $a$ is the **intercept** and $b$ is the **slope**.

This equation encodes a lot of useful information. 

### Equation of the line of best fit: the intercept

$$y = a + bx$$

If $x=0$, the equation says that $y=a$, which is why $a$ is known as the intercept. 

* Is the value of the intercept always meaningful? 

### Equation of the line of best fit: the slope

$$y = a + bx$$

$b$ is known as the slope because an increase from $x$ to $x+1$ is associated with an increase in $y$ 
by the amount $b$.

The slope is closely related to the correlation coefficient:

$$b=r\frac{s_y}{s_x}$$

Note that this means that the correlation coefficient and the $b$ will always have the same sign.

### The R-squared value

The $r^2$ value or R-squared, is the fraction of the variation in the values of $y$ that is explained by the regression of $y$ on $x$

If all points in a scatter plot between X and Y fall exactly on the regression line, the value of $r^2$ is 1.

### Fitting a linear model in R

**Code to run a linear model:** `lm(y ~ x, data = your_data)`

- `lm()` is the function for a linear model.
- The first argument that `lm()` wants is a **formula**: `y ~ x`.
    - `y` is the **response variable** from your dataset/what you are trying to predict
    - `x` is the **explanatory variable**/what you are using to make a prediction
    - be careful with the order of x and y! It is opposite from the default order in ggplot when we write `ggplot(data, aes(x = your_x, y = your_y))`
- The second argument sent to `lm()` is the data set.
    - the default order or declaring the data as the second argument in `lm()` is different from `ggplot2` and `dplyr` functions

### Fitting a linear model in R

Code template:

```{r lm-template, eval=F}
# Students, if you copy this code chunk, you need to set eval = T in the code chunk header for the code to compile

your_lm <- lm(formula = y ~ x, data = your_dataset)

library(broom) # This package makes the output from the linear model look clean 
tidy(your_lm) # This function from the broom package tidies up the output that is printed to the screen
```

### Why the package broom?

```{r, echo=F, fig.align='center'}
knitr::include_graphics("../Images/Ch04_Magic-Broom.png")
```

* `broom` has functions that make the output from the linear model look clean 
* `tidy` is a function from the `broom` package that tidies up the output

```{r, eval=F}
# Students, if you copy this code chunk, you need to set eval = T in the code chunk header for the code to compile

your_lm <- lm(formula = y ~ x, data = your_dataset)

library(broom)
tidy(your_lm) 
```

### Manatee deaths and powerboat purchases 

Let's apply the `lm()` function. Recall the manatee example from Ch.3 that 
examined the relationship between the number of registered `powerboats` and the 
number of manatee `deaths` in Florida between 1977 and 2016.

```{r load-mana-data, echo=F, message=F}
library(readr)
mana_data <- read_csv("../Data/Ch03_Manatee-deaths.csv")
```

Recall that the relationship was linear by examining the scatter plot:

```{r scatter-plot-mana, out.width="80%"}
library(ggplot2)
ggplot(mana_data, aes(x = powerboats, y = deaths)) +
  geom_point() +
  theme_minimal(base_size = 15)
```

### lm() of manatee deaths and powerboat purchases

Calculate the line of best fit: 

```{r run-linear-model-mana}
mana_lm <- lm(deaths ~ powerboats, mana_data)

library(broom) 
tidy(mana_lm) 
```

- Only pay attention to the "term" and "estimate" columns for now.

**Interpret the model output**

- **Intercept**: The predicted number of deaths if there were no powerboats. But the
prediction is negative. Why?
- **powerboats**: This is the **slope** of the line. It is labelled "powerboats" because in more 
advanced models we can have multiple `X` explanatory variables and have a slope
for each one. 
- Question: What does the estimated slope for powerboats mean?

### Interpret the model output 

- Question: What does the estimate slope for powerboats mean?
- Answer: A one unit change in the number of powerboats registered (X 1,000) is 
associated with an increase of manatee deaths of 0.1358. That is, an increase in 
the number of powerboats registered by 1,000 is association with 0.1358 more 
manatee deaths.

### Participation question!

### Add the regression line to the scatter plot using `geom_abline()`

- What parameters do we pass `geom_abline()`?
- Notice that we still cannot see the y intercept in this plot. This is because ggplot only shows the plotting region that corresponds to the range of the data. That is, the y axis does not extend to zero in this plot
```{r scatter-with-line-of-best-fit, out.width="80%"}
# students, know how to use geom_abline() to add a line to scatterplot
# students do not need to know the coord_fixed command.

default_zoom_plot <- ggplot(mana_data, aes(x = powerboats, y = deaths)) +
  geom_point() + 
  labs(x = "Powerboats registered (X 1000)",
       y = "Manatee deaths") +
  geom_abline(intercept = -46.7520, slope = 0.1358) + 
  coord_fixed(ratio = 5) +
  theme_minimal(base_size = 15)

default_zoom_plot
```

### Change the plotting region to show the y intercept

- Now we can see the intercept estimate. It is where the line of best fit 
intersects the y axis. Should we interpret it?
- It is far from the bulk of the data, there is no data near powerboats = 0
- Interpretation would be **extrapolation**, which is **not supported** by these data

```{r zoom-out-plot, out.width="80%", echo = F}
# The code below (not shown in the slides) changes the range of the X and Y axes
# So that we can see the y intercept. It also adds green axes lines.
# Students don't need to know how to change the plotting region, but it may help them when interpeting their regression models!

zoom_out_plot <- ggplot(mana_data, aes(x = powerboats, y = deaths)) +
  geom_hline(yintercept = 0, col = "forest green") + 
  geom_vline(xintercept = 0, col = "forest green") +
  geom_point() + 
  labs(x = "Powerboats registered (X 1000)",
       y = "Manatee deaths") +
  geom_abline(intercept = -46.7520, slope = 0.1358) + 
  scale_x_continuous(limits = c(0, 1050)) + 
  scale_y_continuous(limits = c(-50, 110)) + 
  coord_fixed(ratio = 6) +
  theme_minimal(base_size = 15)

zoom_out_plot
```

### R-squared

- When we run a linear model, the r-squared is also calculated. 
- `glance()` is a function from `broom`. It shows the r-squared for the manatee data:

```{r, glance-at-model}
glance(mana_lm)
```

Focus on:

- Column called `r.squared` only.
- Interpretation of r-squared: 89.3% of the variable in manatee deaths is explained by 
variation in the number of motorboats.

### Example using transformed data

- Sometimes, the data is transformed to another scale so that the relationship
between the transformed $x$ and $y$ is linear
- Table 3.4 in the textbook provides data on the mean number of seeds produced in a year 
by several common tree species and the mean weight (im milligrams) of the seeds 
produced. 

```{r make-seed-dataset}
# Students, you don't need to know how to make a tibble data frame
# Just know how to look at this code and see that a data frame is being created.
library(tibble)
seed_data <- tribble(~ species, ~ seed_count, ~ seed_weight,
                       "Paper birch", 27239, 0.6,
                       "Yellow birch", 12158, 1.6,
                       "White spruce", 7202, 2.0,
                       "Engelman spruce", 3671, 3.3, 
                       "Red spruce", 5051, 3.4, 
                       "Tulip tree", 13509, 9.1, 
                       "Ponderosa pine", 2667, 37.7, 
                       "White fir", 5196, 40.0, 
                       "Sugar maple", 1751, 48.0, 
                       "Sugar pine", 1159, 216.0, 
                       "American beech", 463, 247, 
                       "American beech", 1892, 247,
                       "Black oak", 93, 1851, 
                       "Scarlet oak", 525, 1930, 
                       "Red oak", 411, 2475, 
                       "Red oak", 253, 2475,
                       "Pignut hickory", 40, 3423, 
                       "White oak", 184, 3669, 
                       "Chestnut oak", 107, 4535)
```

### Scatter plot of `seed_weight` vs. `seed_count`

```{r, out.width="80%"}
ggplot(seed_data, aes(seed_count, seed_weight)) + 
  geom_point() +
  theme_minimal(base_size = 15)
```

- `seed_count` and `seed_weight` both vary widely
- Their relationship is not linear

### Investigate the relationship between their logged variables

- Add transformed variables to the dataset using `mutate()`. 
- We add both log base $e$ and log base 10 variables for illustration

```{r calc-logged-vars, message=F, warning=F}
library(dplyr)
seed_data <- seed_data %>% mutate(log_seed_count = log(seed_count),  # base e
                                  log_seed_weight = log(seed_weight), # base e
                                  log_b10_count = log(seed_count, 10), # base 10
                                  log_b10_weight = log(seed_weight, 10)) # base 10
```

### Plot transformed data (log base e)

```{r scatter-logged, out.width="50%"}
ggplot(seed_data, aes(log_seed_count, log_seed_weight)) + 
  geom_point() +
  labs(x = "Log of seed count", y = "Log of seed weight", 
       title = "Using the natural log (base e)") +
  theme_minimal(base_size = 15)
```

### Plot transformed data (log base 10)

```{r scatter-log-base10, out.width="50%"}
ggplot(seed_data, aes(log_b10_count, log_b10_weight)) + 
  geom_point() +
  labs(x = "Log of seed count", y = "Log of seed weight", 
       title = "Using log base 10") +
  theme_minimal(base_size = 15)
```

- You can use either base 10 or base $e$ for class. 
- The calculations using base $e$ are easier 

### `lm()` on the log (base e) variables

```{r run-linear-model-seed}
seed_mod <- lm(log_seed_weight ~ log_seed_count, data = seed_data)
tidy(seed_mod)
glance(seed_mod) %>% pull(r.squared)
```

- Interpret the intercept:
- Interpret the slope: 

### `lm()` on the log (base 10) variables

```{r run-linear-model-seed-b10}
seed_mod_b10 <- lm(log_b10_weight ~ log_b10_count, data = seed_data)
tidy(seed_mod_b10)
glance(seed_mod_b10) %>% pull(r.squared)
```

- What is different from the log base $e$ output?

### Interpretation of `lm()` when using log (base $e$) data

- We use the results of the `lm()` on the log (base $e$) transformed data for making 
predictions
- E.g., what seed weight is predicted for a seed count of 2000?
- Worked calculation:

1. Write down the line of best fit: $log_e(seed.weight) = 15.49130 - 1.522220\times{log_e(seed.count)}$
2. Plug in $seed.count = 2000$ into the line of best fit: $log_e(seed.weight) = 15.49130 - 1.522220\times{log_e(2000)}$
3. Solve for seed count by exponentiating both sides: 
$$seed.weight = exp(15.49130 - 1.522220\times{log_e(2000)})$$ (this uses the property that $e^{log_e(x)}=x$)
$$seed.weight = 50.45$$
4. Interpret: Seeds are expected to weigh 50.45 mg for trees having a seed count of 2000.

**Make sure you can do this worked calculation on a calculator that you will bring to the midterm.**

**No graphing calculators will be permitted.**

### How do outliers affect the line of best fit?

To study this, we use data from the Organisation for Economic Co-operation and 
Development (OECD). This dataset was downloaded from http://dx.doi.org/10.1787/888932526084 
and contains information on the health expenditure per capita and the GDP per
capita for 40 countries. 

```{r read-spending-data}
library(readxl)

spending_dat <- read_xlsx("../Data/Ch04_Country-healthcare-spending.xlsx", 
                          sheet = 2,
                          range = "A7:D47")
```

### Have a look

Next, we want to examine the imported data to see if it is how we expect:
```{r examine-spending-data}
str(spending_dat)
head(spending_dat)
```

### Rename() some variables to use a consistent naming style

If the variable name has spaces, we must use back ticks when referring to it:

```{r rename-variables-with-spaces}
library(dplyr)
spending_dat <- spending_dat %>% 
  rename(country_code = Country.code,
         health_expenditure = `Health expenditure per capita`, # back ticks
         GDP = `GDP per capita`) # back ticks
```

### Examine the relationship

Make a scatter plot of `health_expenditure` (our response variable) vs. each country's level of `GDP`:

```{r plot-spending-data}
install.packages("ggrepel")
library(ggrepel) #this library is used for adding labels to a scatter plot that don't overlap the data points

ggplot(spending_dat, aes(x = GDP, y = health_expenditure)) + 
  geom_point() +
  geom_text_repel(aes(label = country_code)) +
  theme_minimal(base_size = 15)
```

### Examine the relationship

Is the relationship linear? Which countries are outliers? 

Fit a linear model to these data and add it to the graph:

```{r fit-lm-and-plot}
lm(health_expenditure ~ GDP, data = spending_dat)

ggplot(spending_dat, aes(x = GDP, y = health_expenditure)) + 
  geom_point() +
  geom_text_repel(aes(label = country_code)) + # this adds the country code as a label 
  geom_abline(intercept = 44.65623, slope = 0.09399, lty = 2) +
  theme_minimal(base_size = 15)
```

### Examine the relationship without Luxembourg in the data

Let's see whether removing Luxembourg changes the fit of the line. We can remove 
Luxembourg using the `filter()` command from `dplyr`:

```{r remove-lux-and-save-new-dataset}
spending_dat_no_LUX <- spending_dat %>% filter(country_code != "LUX")

lm(health_expenditure ~ GDP, data = spending_dat_no_LUX)

ggplot(spending_dat, aes(x = GDP, y = health_expenditure)) + geom_point() +
  geom_text_repel(aes(label = country_code)) + 
  geom_abline(intercept = 44.65623, slope = 0.09399, lty = 2) + 
  geom_abline(intercept = -785.1044, slope = 0.1264, col = "red") +
  theme_minimal(base_size = 15)
```

### Examine the relationship without USA in the data

```{r remove-usa-and-save-new-dataset}
spending_dat_no_USA <- spending_dat %>% filter(country_code != "USA")

lm(health_expenditure ~ GDP, data = spending_dat_no_USA)

ggplot(spending_dat, aes(x = GDP, y = health_expenditure)) + geom_point() +
  geom_text_repel(aes(label = country_code)) + 
  geom_abline(intercept = 44.65623, slope = 0.09399, lty = 2) + 
  geom_abline(intercept = 152.26274, slope = 0.08714, col = "blue") +
  theme_minimal(base_size = 15)
```

### Examine the relationship without LUX or USA in the data

Let's write the code together to remove both the USA and LUX and see how it 
affects the fit:

```{r remove-usa-lux}
spending_dat_no_USA_LUX <- spending_dat %>% filter(country_code != "USA" & country_code != "LUX")

#alternatively, you could have written:
spending_dat_no_USA_LUX <- spending_dat %>% filter(! country_code %in% c("USA", "LUX"))

#pick the filter command that makes the most sense to you.

lm(health_expenditure ~ GDP, data = spending_dat_no_USA_LUX)

ggplot(spending_dat_no_USA_LUX, aes(x = GDP, y = health_expenditure)) + geom_point() +
  geom_text_repel(aes(label = country_code)) + 
  geom_abline(intercept = 44.65623, slope = 0.09399, lty = 2) + 
  geom_abline(intercept = -592.6973, slope = 0.1166 , col = "green") +
  theme_minimal(base_size = 15)
```

What would happen if USA's point had actually been along the original line of 
best fit (say at x = 80000 and y = 7500) and we re-fit the line without USA's 
point? Would USA have been an **outlier**? Would it be considered **influential**?

### But, is it causal? 

- Creating a scatter plot and a simple linear model is an important step in many 
analyses. It allows you to see the relationship between two quantatitive 
variables and estimate the line of best fit. 

- Sometimes these relationships will be used to make claims of causality. Baldi 
& Moore emphasize that experiments are the best way to study causality. While 
this is often true, sophisticated causal methods have been developed for the 
analysis of observational data. 

### Discussion of some examples from Baldi & Moore

**Example 4.7 "Nature, nuture, and lurking variables"** presents an 
advertisement from the Michigan Symphony:

"Question: Which students scored 51 points higher in verbal skills and 39 points 
higher in math? 

Answer: Students who had experience in music."

Marketers often make leading statements that make their product or service sound
appealing. The purpose of this ad was to have the target audience impute that 
music causes higher marks at school because there is an association between 
enrollment in music and higher marks. However, are students enrolled in music 
lessons otherwise the same as students not enrolled in music lessons? What else
do you expect to differ between these groups of students?

### Discussion of some examples from Baldi & Moore

We can encode these differences in a causal diagram. Here is a simple one to demonstrate the concept:

```{r dag, echo=F, fig.height=2, fig.width=4}
#students, you don't need to know the dagitty package or understand this code
install.packages("dagitty")
library(dagitty)

g <- dagitty('dag {
    Music.Lessons [pos="0,1"]
    High.grades [pos="2,1"]
    Family.income [pos="1,0.5"]
    
    Music.Lessons -> High.grades 
    Family.income -> Music.Lessons
    Family.income -> High.grades
}')
plot(g)

```

The forking at the "Family Income" node makes explicit that we believe family 
income to be a confounder of the relationship between taking music lessons and 
achieving higher grades. It means that not only do these children take music 
lessons, they also come from families with higher incomes, and higher incomes 
lead to higher grades in other ways. Of course, family income is not the only 
possible confounder. What are some others? 

### Confounding

In this course, we don't address how to control for confounding or other types 
of bias that limit causal interpretations. However, know that causality can be 
studied using observational data and relies on clever study designs and 
oftentimes on advanced methods. 









